# AlphaZero on a Connect 4 environment

A deep reinforcement learning algorithm that plays Connect 4, based on AlphaZero. I'm creating this because [my chess algorithm](https://github.com/zjeffer/chess-deep-rl-cpp) learns too slowly, and I wanted to know if the problem is the amount of data needed, or my implementation of the algorithm itself.

## TODO

* [X] Connect 4 environment
* [X] MCTS algorithm
* [X] Neural network
* [X] AlphaZero self-play
* [X] Argument parsing
* [ ] Load settings from file
* [X] Unit tests:
  * [X] Horizontal win
  * [X] Vertical win
  * [X] Diagonal win
  * [X] Easy puzzle
  * [ ] Harder puzzle
  * [ ] ...?
* [X] Save played moves to memory, and memory to file
* [X] AlphaZero training
* [ ] AlphaZero evaluation
* [ ] Automatic pipeline for selfplay, training and evaluation
* [ ] Play against computer
* [ ] GUI?


[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fzjeffer%2Fconnect4-deep-rl&count_bg=%235E81AC&title_bg=%23555555&icon=&icon_color=%235E81AC&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
